{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/diezejhon/the-voice-of-time-predicting-podcast-listening?scriptVersionId=236849981\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<center>\n  <h1 style=\"background-color:#162B48; padding: 8px; color:white\"><strong>üéßüîçThe Voice of Time: Predicting Podcast Listening</strong></h1>\n\n  <img src=\"https://wintringham.org/wp-content/uploads/2020/01/listening-to-the-wintringham-podcast-scaled.jpg\" width=\"800\"/>\n\n<center> \n    \n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n\n  <h3 class=\"list-group-item list-group-item-action active\" style=\"background-color:#162B48; color:white\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"><strong>Summary</strong></h3> \n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#guidingquestion\" role=\"tab\" aria-controls=\"profile\" style=\"color:#162B48\">Guiding Question<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#context\" role=\"tab\" aria-controls=\"profile\" style=\"color:#162B48\">Context<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">2</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#goal\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">Goal<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">3</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#eda_univariate\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">EDA Univariate<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">4</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#eda_multivariate\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">EDA Multivariate<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">5</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#preprocessing\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">Preprocessing<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">6</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#models\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">Models<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">7</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#submission\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">Submission<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">8</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#conclusion\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">Conclusion<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">9</span></a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#references\" role=\"tab\" aria-controls=\"messages\" style=\"color:#162B48\">References<span class=\"badge badge-primary badge-pill\" style=\"background-color:#162B48; color:white\">10</span></a>\n</div>    \n<br><br>\n<div style=\"font-family: Helvetica; line-height: 1.75; color: #000000; text-align: justify\">\n\nThis notebook was developed in collaboration with <a style=\"color: #462F53\" href=\"https://www.kaggle.com/diezejhon\"> @diezejohn</a> and <a style=\"color: #462F53\" href=\"https://www.kaggle.com/feeldidaxie\"> @feeldidaxie</a>. If you found it useful, we‚Äôd appreciate your support on both notebooks!\n<br><br>\n <li> DiezeJohn notebook: <a style=\"color: #462F53\" href=\"https://www.kaggle.com/code/diezejhon/the-voice-of-time-predicting-podcast-listening\"> üéßüîçThe Voice of Time: Predicting Podcast Listening by diezejohn</a>\n <li> Feel Didaxie notebook: <a style=\"color: #462F53\" href=\"https://www.kaggle.com/code/feeldidaxie/the-voice-of-time-predicting-podcast-listening\"> üéßüîçThe Voice of Time: Predicting Podcast Listening by Feel Didaxie</a>\n</div>   ","metadata":{}},{"cell_type":"markdown","source":"<a id='guidingquestion'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>1. | Guiding Question</strong></div>\n<div style=\"font-family: Helvetica; line-height: 1.75; color: #000000; text-align: justify\">\n\nBefore diving into the heart of the matter, it is important <span style=\"background-color: #F6CECE; color: #000000;\">\n<strong>to define the problem we want to solve</strong></span>. No matter the project, we are always looking to solve a problem. This problem can vary‚Äîimproving security, improving resource management, reducing costs, and so on.\n<br><br>\n<strong><u>Example:</u></strong> \nLet‚Äôs imagine that we are working for an audio streaming platform (Spotify, YouTube Music, Deezer, etc.) and <span style=\"background-color: #F6CECE; color: #000000;\">we want to improve our podcast recommendations.</span>\n<br><br>\nOne idea we can develop is: <span style=\"background-color: #F6CECE; color: #000000;\">for a given user, predict the listening time of a podcast X.</span> If we estimate that the user listens to more than half of the podcast, <u>it makes sense to recommend that podcast to them.</u>\n<br><br>\nFor example, for user 1000, the model <strong>estimates</strong> that they will <strong>listen to 20 minutes</strong> of the first episode of the ‚ÄúTech News‚Äù podcast, <strong>which lasts 25 minutes</strong>. <span style=\"background-color: #F6CECE; color: #000000;\">\n<strong>This means the user will listen to 80% of the podcast.</strong></span> Therefore, the user is <u>interested</u> in this podcast, and <strong>it makes sense to recommend it</strong> to them.\n<br><br>\nTo implement our idea, we can consider that if the user listens to <strong>50%</strong> of the podcast, we will recommend it to them. We can do this by creating a new variable called ‚ÄúRecommended,‚Äù with values ‚ÄúYes‚Äù and ‚ÄúNo.‚Äù This new variable can later be used in our recommendation algorithm.\n<br><br>\nNow that we‚Äôve developed the idea, it‚Äôs time to put it into practice <span style=\"background-color: #F6CECE; color: #000000;\">by collecting the data.</span> The data will be retrieved from the company‚Äôs database, usually in SQL, and then imported into Python.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='context'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>2. | Context</strong></div>\n\n## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong> Interpretation of variables üßæ </strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 3; color: #000000; text-align: justify\">\n\n<center>\n<table style=\"width:80%\">\n  <thead>\n    <tr style = \"color : #262F3A\">\n      <th style=\"text-align: center; font-weight: bold; font-size: 15px; background-color: #F6CECE; line-height: 2;\">Name of variables</th>\n      <th style=\"text-align: center; font-weight: bold; font-size: 15px; background-color: #F6CECE; line-height: 2;\">Description</th>\n      <th style=\"text-align: center; font-weight: bold; font-size: 15px; background-color: #F6CECE; line-height: 2;\">Example</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr style= \"color: #262F3A\">\n       <td style= \"line-height: 2.5;\"><strong>Podcast_Name</strong></td>\n      <td>Name of the podcast</td>\n      <td>Tech Talk ; Health Hour ; Comedy Central</td>\n    </tr>\n    <tr style=\"background-color: #F6CECE; color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Episode_Title</strong></td>\n      <td>Title of the podcast episode</td>\n      <td>The Future of AI ; Meditation Tips</td>\n    </tr>\n    <tr style=\"color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Episode_Length</strong></td>\n      <td>Length of the episode <br> (in minutes)</td>\n      <td>5.0 ; 30.0 ; 60.0</td>\n    </tr>\n    <tr style=\"background-color: #F6CECE; color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Genre</strong></td>\n      <td>Genre of the podcast episode</td>\n      <td>Technology ; Comedy ; Health</td>\n    </tr>\n    <tr style= \"color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Host_Popularity</strong></td>\n      <td>Popularity of the host <br> (scale: 0 to 100)</td>\n      <td>50.0 ; 75.0 ; 90.0</td>\n    </tr>\n    <tr style=\"background-color: #F6CECE; color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Publication_Day</strong></td>\n      <td>Day of the week the episode was published</td>\n      <td>Monday ; Wednesday ; Saturday</td>\n    </tr>\n    <tr style= \"color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Publication_Time</strong></td>\n      <td>Time of the day the episode was published</td>\n      <td>Morning ; Evening ; Night</td>\n    </tr>\n    <tr style=\"background-color: #F6CECE; color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Guest_Popularity</strong></td>\n      <td>Popularity of the guest (if any) <br> (scale: 0 to 100)</td>\n      <td>20.0 ; 50.0 ; 85.0</td>\n    </tr>\n    <tr style= \"color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Number_of_Ads</strong></td>\n      <td>Number of ads in the episode</td>\n      <td>0 ; 1 ; 2 ; 3</td>\n    </tr>\n    <tr style=\"background-color: #F6CECE; color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Episode_Sentiment</strong></td>\n      <td>Overall sentiment of the episode content</td>\n      <td>Positive ; Neutral ; Negative</td>\n    </tr>\n    <tr style= \"color: #262F3A\">\n      <td style= \"line-height: 2.5;\"><strong>Listening_Time</strong></td>\n      <td>Average actual listening time <br> (target variable, in minutes)</td>\n      <td>4.5 ; 8.0 ; 30.0 ; 60.0</td>\n    </tr>\n  </tbody>\n</table>\n</center>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id='goal'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>3. | Goal</strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 1.75; color: #000000; text-align: justify\">\n\nOur goal is <span style=\"background-color: #F6CECE; color: #000000;\">\n<strong>to predict, for a given user, the listening duration of a podcast X as accurately as possible.</strong></span>\n<br><br>\nFor example, if <u>user 200 listens to the podcast ‚ÄúTech News‚Äù for 25 minutes</u>, our model should ideally <u>predict a duration close to 25 minutes</u>.\n<br><br>\nTo evaluate the performance of our regression model, we use <strong>the Root Mean Squared Error (RMSE).</strong>\n<br><br>\n<u>RMSE</u> corresponds to the square root of the average of the squared prediction errors. It quantifies, in minutes, the average difference between the predicted values and the actual values.\n<br><br>\nFor instance, <span style=\"background-color: #F6CECE; color: #000000;\">an RMSE of 10 means that, on average, the model is off by ¬±10 minutes.</span> In other words, for a prediction of <strong>60 minutes</strong>, the actual duration could reasonably <strong>fall between 50 and 70 minutes</strong>.\n<br><br>\nThe lower the RMSE, the more accurate the model, thus improving the quality of our recommendations.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='eda_univariate'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>4. | EDA Univariate </strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 1.75; color: #000000; text-align: justify\">\n\nIn this section, <u>we will analyze the data to better understand it.</u><br>\nThe goal is to assess how <strong>the data is distributed</strong>, identify any <strong>outliers</strong> or <strong>missing values</strong>, and determine whether we need <strong>to simplify certain variables</strong> through feature engineering.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>4.1 | Import libraries üìö  </strong></div>","metadata":{}},{"cell_type":"code","source":"# --- Installing Libraries ---\n!pip install ydata-profiling\n!pip install Pillow","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Handling data -----\nimport pandas as pd\nimport numpy as np\n\n\n# ----- Graphics -----\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle\nimport matplotlib.colors as mcolors\nfrom matplotlib.lines import Line2D\n\n# ----- EDA Univariate -----\nfrom ydata_profiling import ProfileReport\n\n\n# ----- Remove the warnings -----\nimport warnings","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove the warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>4.2 | Import the data</strong></div>","metadata":{}},{"cell_type":"code","source":"# ----- Read the dataset -----\ndf_train = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv', index_col=\"id\")\ndf_test = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv', index_col=\"id\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head(3).style.background_gradient(cmap='Blues').hide(axis=\"index\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head(3).style.background_gradient(cmap='Blues').hide(axis=\"index\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>4.3 | Dataset Exploration</strong></div>","metadata":{}},{"cell_type":"code","source":"# --- Convert Episode Title into category ---\ndf_train[\"Episode_Title\"] = df_train[\"Episode_Title\"].astype(\"category\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Dataset Report with ProfileReport for the train set -----\n\nProfileReport(df_train, title='Train Dataset', \n              minimal = False, \n              progress_bar = False, \n              samples = None, \n              interactions = None,\n              correlations = None,\n              explorative = True,\n              notebook = {'iframe':{'height': '600px'}},\n              missing_diagrams = {'heatmap': False, 'dendrogram': True}).to_notebook_iframe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n\n\n<span style=\"background-color: #F6CECE; color: #000000; font-size: 18px;\"><strong><u> Overview:</u></strong></span>\n<br><br>    \nWe have <strong>11 variables</strong>, with <strong>750,000 podcast listen</strong>s and <strong>2.8% missing data</strong>.\n<blockquote style=\"color: #000000;\">\n<li><u><strong>Podcast Name:</strong></u> There are <strong>48</strong> different podcast names. If we encode this variable, we would add 48 new features to our model, which could cause dimensionality issues. It would be wise <span style=\"background-color: #F6CECE; color: #000000;\">to either remove or simplify it</span> ‚Äî for example, by grouping into 6 categories instead of 48.</li>\n\n<li><u><strong>Episode Title:</strong></u> There are <b>100</b> different episodes, ranging from Episode 1 to Episode 100. As with ‚ÄúPodcast_Name,‚Äù we will either need <span style=\"background-color: #F6CECE; color: #000000;\">to simplify or remove</span> this variable.</li>\n\n<li><u><strong>Episode Length minutes:</strong></u> On <strong>average</strong>, an episode lasts <strong>64 minutes</strong>. However, <strong>the distribution is uniform</strong>, meaning all durations are properly represented (there is no underrepresentation). Episode lengths range from <strong>0 to 325.24 minutes</strong>. <span style=\"background-color: #F6CECE; color: #000000;\">Episodes longer than 119.99 minutes are considered outliers</span>, as they account for only 9 listens. Additionally, <strong>11.6% of the values are missing</strong>.</li>\n\n<li><u><strong>Genre:</strong></u> There are <strong>10</strong> different genres. The most listened-to genre is ‚ÄúSports,‚Äù followed by ‚ÄúTechnology.‚Äù</li>\n\n<li><u><strong>Host Popularity Percentage:</strong></u> On <strong>average</strong>, the podcast host has a popularity rating of <strong>60%</strong>. However, <strong>the distribution is uniform</strong>, meaning there are both very popular and unknown hosts. Values range from <strong>0% to 119%</strong>, indicating the presence of <strong>outliers</strong>.</li>\n\n<li><u><strong>Publication Day:</strong></u> There are <strong>7</strong> different days, <strong>distributed uniformly</strong>. Podcasts are just as likely to be published on a Monday as on a Sunday.</li>\n\n<li><u><strong>Publication Time:</strong></u> There are <strong>4</strong> different publication times, <strong>distributed uniformly</strong>. Podcasts are just as likely to be published in the morning as in the evening.</li>\n\n<li><u><strong>Guest Popularity Percentage:</strong></u> On <strong>average</strong>, a podcast guest has a popularity rating of <strong>52%</strong>. However, <strong>the distribution is uniform</strong>, with both well-known and unknown guests. <strong>19.5%</strong> of the values are missing, possibly indicating episodes without guests. Values range from <strong>0% to 119%</strong>, meaning there are <strong>outliers</strong>.</li>\n\n<li><u><strong>Number of Ads:</strong></u> The number of ads during a listen ranges from <strong>0 to 103 ads</strong>. However, only <strong>7</strong> listens had <strong>more than 3 ads</strong>, which are considered <strong>outliers</strong>. <strong>60%</strong> of listens had 0 or 1 ad. There is 1 missing value.</li>\n\n<li><u><strong>Episode Sentiment:</strong></u> There are <strong>3</strong> categories, <strong>distributed uniformly</strong>. This means there are as many positive, negative, and neutral episodes.</li>\n\n<li><u><strong>Listening Time minutes:</strong></u> On <strong>average</strong>, users listen to a podcast for <strong>45 </strong>minutes. <span style=\"background-color: #F6CECE; color: #000000;\">50% of users listen to a podcast for between 23 and 64 minutes</span>.</li>\n     </blockquote>\n</div>","metadata":{}},{"cell_type":"code","source":"# ----- Dataset Report with ProfileReport for the test set -----\n\"\"\"\nProfileReport(df_test, title='Test Dataset', \n              minimal = False, \n              progress_bar = False, \n              samples = None, \n              interactions = None,\n              correlations = None,\n              explorative = True,\n              notebook = {'iframe':{'height': '600px'}},\n              missing_diagrams = {'heatmap': False, 'dendrogram': True}).to_notebook_iframe()\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>4.4 | Outlier</strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n\n<u>We will handle outliers for the following variables:</u>\n\n<li><u><strong>Episode Length minutes:</strong></u> Outliers will be replaced with <strong>119.99</strong>.</li>\n\n<li><u><strong>Host Popularity Percentage:</strong></u> Outliers will be replaced with <strong>100</strong>.</li>\n\n<li><u><strong>Guest Popularity Percentage:</strong></u> Outliers will be replaced with <strong>100</strong>.</li>\n\n<li><u><strong>Number of Ads:</strong></u> Outliers will be replaced with <strong>3</strong>.</li>\n\n</div>","metadata":{}},{"cell_type":"code","source":"### -------------------------------- Handling outlier --------------------------------\n\n\n# Limit episode length to a max of 119.99\ndf_train['Episode_Length_minutes'] = df_train['Episode_Length_minutes'].clip(upper=119.99)\nprint(\"Max Episode Length Minutes:\", df_train[\"Episode_Length_minutes\"].max())\n\n# Limit  host popularity percentange to a max of 100%\ndf_train['Host_Popularity_percentage'] = df_train['Host_Popularity_percentage'].clip(upper=100)\nprint(\"Max Host Popularity Percentage:\", df_train[\"Host_Popularity_percentage\"].max())\n\n# Limit  guest popularity percentange to a max of 100%\ndf_train['Guest_Popularity_percentage'] = df_train['Guest_Popularity_percentage'].clip(upper=100)\nprint(\"Max Guest Popularity Percentage:\", df_train[\"Guest_Popularity_percentage\"].max())\n\n# Limit number of ads to a max of 3 ads\ndf_train['Number_of_Ads'] = df_train['Number_of_Ads'].clip(upper=3)\nprint(\"Max Number of Ads:\", df_train[\"Number_of_Ads\"].max())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>4.5 | Feature Engineering</strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n\n<u>We will simplify or remove the following variables:</u>\n\n<li><u><strong>Podcast Name:</strong></u> We will <strong>remove</strong> this variable. (48 categories)</li>\n\n<li><u><strong>Episode Title:</strong></u> We will <strong>simplify this variable</strong> into 4 categories to indicate whether the podcast is from the beginning (episodes 1 to 25), early middle (episodes 26 to 50), late middle (episodes 51 to 75), or end (episodes 76 to 100). (100 categories)</li>\n</div>    ","metadata":{}},{"cell_type":"code","source":"### -------------------------------- Simplifying a variable --------------------------------\n\n\n# Import Library\nimport re\n\n# Extract episode number from title\ndf_train['Episode_Number'] = df_train['Episode_Title'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n\n# Create categories by episode number\ndf_train['Episode_Category'] = pd.cut(df_train['Episode_Number'], \n                                    bins=[0, 25, 50, 75, 100], \n                                    labels=['Start', 'Early Middle', 'Late Middle', 'End'], \n                                    right=True)\n\ndf_train.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>4.6 | Missing values</strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n\n<u>We will handle missing values for the following variables:</u>\n\n<li><u><strong>Episode Length minutes:</strong></u> <strong>11.6%</strong> missing values.</li>\n\n<li><u><strong>Guest Popularity Percentage:</strong></u> <strong>19%</strong> missing values.</li>\n\n<li><u><strong>Number of Ads:</strong></u> We will <strong>delete</strong> the single missing value.</li>\n<br><br>\n<u><strong>Episode Length minutes:</strong></u>\n\nTo fill in the missing episode duration values, we could use <span style=\"background-color: #F6CECE; color: #000000;\"><strong>a reference value</strong></span> associated with each episode.\n\nFor example, if the podcast <strong>\"Mystery Matters\"</strong> episode <strong>98</strong> lasts <strong>35</strong> minutes, for the same episode, <span style=\"background-color: #F6CECE; color: #000000;\">we could simply replace missing values with ‚Äú35‚Äù.</span>\n    \n</div>","metadata":{}},{"cell_type":"code","source":"# Show null values for Episode Length Minutes\ndf_train[\n    (df_train['Podcast_Name'] == \"Mystery Matters\") & (df_train['Episode_Length_minutes'].isna())\n].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show not null values for Episode Length Minutes\ndf_train[\n    (df_train['Podcast_Name'] == \"Mystery Matters\") & (df_train['Episode_Length_minutes'].notna())\n].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n\nUnfortunately, <u>we observe that for the same episode of the same podcast</u>, the recorded durations vary significantly. <strong>There must be an error in the data.</strong>\nSince we cannot contact the Data Engineer, we must find another solution.\n<br><br>\n<u>To handle the missing episode duration values, we will test two methods:</u>\n\n<li>KNN (K-Nearest Neighbors) imputation</li>\n\n<li>Median imputation based on episode duration</li>\n<br>\n<u>To determine which method is more effective, we will conduct a test:</u>\n<br>\nWe will take <strong>a sample of 10,000 entries</strong>, artificially <strong>remove 2,000 known</strong> episode duration values, and <strong>then apply KNN and median imputation</strong>. After processing, <span style=\"background-color: #F6CECE; color: #000000;\"><strong>we will compare the predicted values to the actual ones and choose the method that results in the lowest error</strong></span>.\n</div>","metadata":{}},{"cell_type":"code","source":"# ----- Handling missing values -----\nfrom sklearn.impute import KNNImputer\n\n# ----- Evaluating the quality of imputation -----\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### -------------------------------- 1. Test for missing values --------------------------------\n\n\n### ---------------- 2. Setting up the test ----------------\n\n# Take non-missing values\ndf_complete = df_train[df_train['Episode_Length_minutes'].notna()].copy()\n\n# Take a sample of 10,000 users\ndf_sample = df_complete.sample(n=10_000, random_state=42).copy()\n\n# Take only independent variables\ndf_sample = df_sample[[\"Episode_Length_minutes\", \"Genre\", \"Host_Popularity_percentage\",\n            \"Publication_Day\", \"Publication_Time\", \"Guest_Popularity_percentage\",\n            \"Number_of_Ads\", \"Episode_Sentiment\", \"Episode_Category\"]]\n\n# Encode qualitative variables into quantitative ones\ndf_sample_encoded = pd.get_dummies(data = df_sample , drop_first = True)\n\n# Make a copy\ndf_nan = df_sample_encoded.copy()\n\n# Take the index of 20% of the dataset\nmissing_indices = df_nan.sample(frac=0.2, random_state=42).index\n\n# Take indexes and set them to missing values\ndf_nan.loc[missing_indices, 'Episode_Length_minutes'] = np.nan\n### -------------------------------------------------------\n\n\n\n### ---------------- 3. KNN Imputation ----------------\n\n# Take the 10 nearest neighbors\nimputer = KNNImputer(n_neighbors=10)\n\n# Apply KNN\ndf_imputed = pd.DataFrame(imputer.fit_transform(df_nan), \n                          columns = df_nan.columns, \n                          index = df_nan.index)\n\n# Take the values predicted by KNN and the true values\nimputed_values = df_imputed.loc[missing_indices, 'Episode_Length_minutes']\noriginal_values = df_sample_encoded.loc[missing_indices, 'Episode_Length_minutes']\n### -------------------------------------------------------\n\n\n\n### ---------------- 4. Median Imputation ----------------\n\n# Take the median\nmedian_value = df_train['Episode_Length_minutes'].median()\n\n# Fill in missing values (2,000 data out of 10,000 data) with the median\nmedian_imputed_values = df_nan['Episode_Length_minutes'].fillna(median_value, inplace=False)\n\n# Take only the 2,000 data items for evaluation\nmedian_imputed_values = median_imputed_values.loc[missing_indices]\n### -------------------------------------------------------\n\n\n\n### ---------------- 5. Evaluation ----------------\n\n# Evaluate imputation with KNN\nrmse_knn = np.sqrt(mean_squared_error(original_values, imputed_values))\nmae_knn = mean_absolute_error(original_values, imputed_values)\n\n# Evaluate imputation with the median\nrmse_median = np.sqrt(mean_squared_error(original_values, median_imputed_values))\nmae_median = mean_absolute_error(original_values, median_imputed_values)\n\n\n# Display table\nprint(\"\\nüéØ R√©sultats de l'imputation :\\n\")\nprint(\"| Method         |   RMSE   |   MAE   |\")\nprint(\"|-----------------|----------|---------|\")\nprint(f\"| Median        |  {rmse_median:7.2f} |  {mae_median:6.2f} |\")\nprint(f\"| KNN (k=10)      |  {rmse_knn:7.2f} |  {mae_knn:6.2f} |\")\n### -------------------------------------------------------","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n\n\n<span style=\"background-color: #F6CECE; color: #000000;\"><strong>We observe that both methods perform poorly</strong></span>.\n\nOn <strong>average</strong>, the prediction error for a missing value is around <strong>¬±30 minutes</strong>. Given that episodes last at most <strong>120</strong> minutes, <strong>this is a very large error</strong>.\n<br>\nIn other words, if we predict that an episode lasts <strong>60</strong> minutes, <strong>its true value could reasonably be anywhere between 30 and 90 minutes</strong>.\n<br>\nIf this variable is important for predicting a user's listening time, the model‚Äôs performance could be greatly impacted, especially since around <strong>11%</strong> of the values are missing, which is considerable.\n<br><br>\n<span style=\"background-color: #F6CECE; color: #000000;\"><strong>Since the median method is slightly more accurate and faster, we will choose it.</strong></span>\nThus, we will fill the missing values with the median duration from the training dataset.\n<br><br>\n<u><strong>Guest Popularity Percentage:</strong></u>\n<br><br>\n<u>Missing values related to guests could mean several things:</u>\n\n<li>Either there was an issue during data collection, resulting in 19.6% truly missing values.</li>\n\n<li>Or some episodes simply had no guest, and by default, this absence was recorded as missing data.</li>\n<br>\n<span style=\"background-color: #F6CECE; color: #000000;\"><strong>In our case, we will assume that episodes with missing guest popularity had no guest.</strong></span>\n<br>    \nOur strategy for handling these missing values is <strong>to build two models</strong>:\none including the ‚ÄúGuest_Popularity_Percentage‚Äù variable and one excluding it.\n</div>","metadata":{}},{"cell_type":"code","source":"# ------- Fill in missing values -------\n\n# Fill in missing values with the median\ndf_train['Episode_Length_minutes'].fillna(df_train['Episode_Length_minutes'].median(), inplace = True)\n\n# Remove missing value\ndf_train.dropna(subset=['Number_of_Ads'], inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the dataset\ndf_train.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='eda_multivariate'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>5. | EDA Multivariate </strong></div>\n","metadata":{}},{"cell_type":"code","source":"def correlation_matrix_3(df, color_0, color_1, color_2, subtitle_1, subtitle_2, subtitle_3):\n    \n    # Correlation matrix from -1 to 1 (with 3 colors)\n    \n    # If we want a correlation matrix from 0 to 1, we need 2 colors\n    \n    # Import the library: import matplotlib.colors as mcolors\n    \n    # Highlight text properties\n    highlight_textprops = [{\"fontsize\":12, \"color\":f'#{color_0}', \"fontname\": \"Cover sans\", \"fontweight\": \"bold\"},\n                           {\"fontsize\":12, \"color\":f'#33363F', \"fontname\": \"Cover sans\"},\n                           {\"fontsize\":12, \"color\":f'#{color_0}', \"fontname\": \"Cover sans\", \"fontweight\": \"bold\"},]\n\n\n    # Axis labels color\n    variable_name_textprops = [{\"fontsize\":8, \"color\":f'#33363F', \"fontname\": \"Cover sans\", \"fontweight\": \"bold\"}]\n    \n    # Correlation matrix\n    correlation_matrix = df.corr(numeric_only=True)\n\n    # Figure\n    fig, ax = plt.subplots(figsize=(12, 8), dpi=500)\n    \n\n    # Remove the upper half of the matrix\n    mask = np.triu(np.ones_like(df.corr(numeric_only=True), dtype=bool))\n\n    \n    color1 = mcolors.to_rgba(f'#{color_0}')  # Negative value -1\n    color_intermediate = mcolors.to_rgba(f'#{color_1}')  # Intermediate color (Value 0)\n    color2 = mcolors.to_rgba(f'#{color_2}')  # Positive value 1\n\n    \n    # Create a custom color palette\n    n, m = 256, 1\n    cmap_custom = mcolors.LinearSegmentedColormap.from_list('custom', [color1, color_intermediate, color2], N=n, gamma=m)\n\n    # Correlation matrix heatmap\n    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap=cmap_custom, fmt=\".2f\", linewidths=0.2, cbar=False,\n               annot_kws={\"size\": 10})\n    \n    \n    # Horizontal and vertical labels\n    xy_label = dict(size=6)\n    yticks, ylabels = plt.yticks()\n    xticks, xlabels = plt.xticks()\n    ax.set_xticklabels(xlabels, rotation=0, **xy_label, **variable_name_textprops[0])\n    ax.set_yticklabels(ylabels, **xy_label, **variable_name_textprops[0])\n    \n    \n    # Add a title to the heatmap axis\n    ax.set_title('Correlation of Numerical Variables', fontsize=20, fontweight='bold', \n             fontname='Lisboa Sans OSF', color = \"#33363F\")\n    \n    \n    # Title\n    # Ajouter du texte √† l'axe avec ax.text()\n    ax.text(0.40, 0.845, f\"{subtitle_1} {subtitle_2} {subtitle_3}\", \n        va='bottom', ha='center', fontsize=12, \n        bbox=dict(facecolor='none', edgecolor='none', boxstyle='round,pad=0.3'), \n        color='black')  # Vous pouvez ajuster les propri√©t√©s comme la couleur, la taille de police, etc.\n    \n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline\ncorrelation_matrix_3(df_train,\"243B6E\", \"FFFCF9\", \"EA7F1B\", \"\", \"\", \"\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Correlation analysis between numerical variables\nnumeric_cols = ['Episode_Length_minutes', 'Host_Popularity_percentage',\n                'Guest_Popularity_percentage', 'Number_of_Ads',\n                'Episode_Number', 'Listening_Time_minutes']\n\n# Random sampling\ndf_train_viz = df_train.sample(n=10000, random_state=42)\n\n# 2. Interaction: host popularity, genre, and listening time\nplt.figure(figsize=(14, 8))\nsns.scatterplot(data=df_train_viz, x='Host_Popularity_percentage', y='Listening_Time_minutes',\n                hue='Genre', size='Guest_Popularity_percentage', sizes=(20, 200), alpha=0.7)\nplt.title(\"Host Popularity, Genre, and Listening Time\")\nplt.xlabel(\"Host Popularity (%)\")\nplt.ylabel(\"Listening Time (minutes)\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# 3. Impact of publication day and number of ads\nplt.figure(figsize=(14, 8))\nsns.boxplot(data=df_train_viz, x='Publication_Day', y='Listening_Time_minutes', hue='Number_of_Ads')\nplt.title(\"Listening Time by Publication Day and Number of Ads\")\nplt.xlabel(\"Publication Day\")\nplt.ylabel(\"Listening Time (minutes)\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 4. Listening time / episode length ratio by sentiment and category\ndf_train_viz['Listening_Ratio'] = df_train_viz['Listening_Time_minutes'] / df_train_viz['Episode_Length_minutes']\n\nplt.figure(figsize=(16, 10))\nsns.boxplot(data=df_train_viz, x='Episode_Category', y='Listening_Ratio', hue='Episode_Sentiment')\nplt.title(\"Listening Ratio by Episode Category and Sentiment\")\nplt.xlabel(\"Episode Category\")\nplt.ylabel(\"Listening Time / Episode Length Ratio\")\nplt.xticks(rotation=45)\nplt.axhline(y=1, color='red', linestyle='--', alpha=0.7)  # Reference line at 100%\nplt.tight_layout()\nplt.show()\n\n# Define the order of the days of the week\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# Create the pivot table with specific order for Publication_Day\npivot_table = df_train_viz.pivot_table(values='Listening_Time_minutes',\n                                       index='Publication_Day',\n                                       columns='Episode_Category',\n                                       aggfunc='mean',\n                                       observed=True)\n\n# Reindex to order the days\npivot_table = pivot_table.reindex(day_order)\n\n# Create the heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='.1f', linewidths=0.5)\nplt.title(\"Average Listening Time by Day and Category\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"Publication Day\")\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='preprocessing'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>6. | Preprocessing</strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n\n<span style=\"background-color: #F6CECE; color: #000000;\"><strong>Preprocessing consists of transforming the data to facilitate model training and improve its performance.</strong></span><br> <u>It is divided into three main steps:</u>\n\n<li>Feature Engineering</li>\n\n<li>Pipeline</li>\n\n<li>Data Splitting</li>\n<br><br>\nIn our project, <strong>we aim to train two models</strong>: one that includes the ‚ÄúGuest_Popularity_Percentage‚Äù variable and another without it.\n<br>\nFor now, these two models will be developed separately. Once their training is complete, <span style=\"background-color: #F6CECE; color: #000000;\"><strong>we will implement a routing pipeline (Router) to dynamically select which model to use depending on the context</strong></span>, thereby simplifying deployment to production.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>6.1 | Import libraries üìö</strong></div>","metadata":{}},{"cell_type":"code","source":"# --- Preprocessing ---\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.model_selection import train_test_split","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>6.2 | Feature Engineering</strong></div>","metadata":{}},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>6.3 | Pipeline</strong></div>","metadata":{}},{"cell_type":"code","source":"# ----- Create a transform for a new variable -----\n\ndef add_episode_category_column(df):\n    \n    # Convert to category\n    df = df.copy()\n    df[\"Episode_Title\"] = df[\"Episode_Title\"].astype(\"category\")\n    \n    # Extract episode number\n    df['Episode_Number'] = df['Episode_Title'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n    \n    # Categorize episodes\n    df['Episode_Category'] = pd.cut(df['Episode_Number'], \n                                    bins=[0, 25, 50, 75, 100], \n                                    labels=['Start', 'Early Middle', 'Late Middle', 'End'], \n                                    right=True)\n    return df\n\nepisode_category_transformer = FunctionTransformer(add_episode_category_column, validate=False)\n# --------------------------------------------------","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Handling outliers -----\n\ndef outlier_features(df):\n    df = df.copy()\n    \n    if 'Episode_Length_minutes' in df.columns:\n        df['Episode_Length_minutes'] = df['Episode_Length_minutes'].clip(upper=119.99)\n        \n    if 'Host_Popularity_percentage' in df.columns:\n        df['Host_Popularity_percentage'] = df['Host_Popularity_percentage'].clip(upper=100)\n        \n    if 'Guest_Popularity_percentage' in df.columns:\n        df['Guest_Popularity_percentage'] = df['Guest_Popularity_percentage'].clip(upper=100)\n        \n    if 'Number_of_Ads' in df.columns:\n        df['Number_of_Ads'] = df['Number_of_Ads'].clip(upper=3)\n    \n    return df\n\noutlier_transformer = FunctionTransformer(outlier_features, validate=False)\n# --------------------------------------------------","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Create a transform for missing values -----\n\ndef fill_missing_features(df):\n    df = df.copy()\n    \n    median_val = df['Episode_Length_minutes'].median()\n    df['Episode_Length_minutes'] = df['Episode_Length_minutes'].fillna(median_val)\n    \n    return df\n\nfill_missing_transformer = FunctionTransformer(fill_missing_features, validate=False)\n# --------------------------------------------------","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Create a transform to delete selected variables -----\n\ndef remove_columns(df):\n    cols_to_drop = ['Podcast_Name', 'Episode_Title', 'Episode_Number']\n    existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n    df = df.drop(columns=existing_cols_to_drop)\n    return df\n\nremove_columns_transformer = FunctionTransformer(remove_columns, validate=False)\n# --------------------------------------------------","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Create an encoding transformer -----\n\ndef apply_get_dummies(X):\n    return pd.get_dummies(X, drop_first=True)\n\nencoding = FunctionTransformer(apply_get_dummies, validate=False)\n# --------------------------------------------------","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Create a transformer to standardize quantitative variables -----\n\nclass SelectiveStandardScaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.scaler = StandardScaler()\n    \n    def fit(self, X, y=None):\n        # Automatically select numeric columns\n        self.continuous_columns = X.select_dtypes(include=['float64', 'int64']).columns\n        self.scaler.fit(X[self.continuous_columns])\n        return self\n    \n    def transform(self, X):\n        # Apply scaler only to continuous columns\n        X_scaled = X.copy()\n        X_scaled[self.continuous_columns] = self.scaler.transform(X[self.continuous_columns])\n        return X_scaled\n# --------------------------------------------------","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Preprocessing Pipeline -----\n\npreprocessing_pipeline = Pipeline([\n    \n    (\"episode_category\", episode_category_transformer), # Add \"Episode_Category\"\n    ('outlier', outlier_transformer),                   # Clean outlier\n    ('fill_missing', fill_missing_transformer),         # Fill missing values\n    ('remove_columns', remove_columns_transformer),     # Remove columns\n    (\"encoder\", encoding),                              # Encoding\n    ('scaler', SelectiveStandardScaler())               # Standardisation\n\n])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Testing our pipeline -----\n\ndata = {\n    \"Podcast_Name\" : [\"Broad\"]*5,\n    \"Episode_Title\" : [\"Episode 12\", \"Episode 95\", \"Episode 50\", \"Episode 75\", \"Episode 35\"],\n    'Episode_Length_minutes': [130, 150, 2, 3, 60],\n    'Genre' : [\"True Crime\", \"Comedy\", \"Education\", \"Technology\", \"Health\"],\n    'Host_Popularity_percentage': [30, 150, 2, 3, 600],\n    \"Publication_Day\" : [\"Thursday\", \"Saturday\", \"Tuesday\", \"Monday\", \"Monday\"],\n    'Publication_Time': [\"Night\", \"Afternoon\", \"Evening\", \"Morning\", \"Afternoon\"],\n    \"Guest_Popularity_percentage\" : [30, 150, 2, 3, 600],\n    \"Number_of_Ads\" : [3, 150, 2, 3, 4],\n    \"Episode_Sentiment\" : [\"Positive\", \"Negative\", \"Neutral\", \"Negative\", \"Positive\"]\n}\n\ndata = pd.DataFrame(data)\n\nprocessed_data = preprocessing_pipeline.fit_transform(data)\n\nprint(\"Created data :\")\nprint(data.head())\nprint(\"\\n \\n \\n\")\nprint(\"-\"*100)\nprint(\"\\nData after transformation :\")\nprint(processed_data.head())","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  ","metadata":{}},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>6.4 | Data separation</strong></div>","metadata":{}},{"cell_type":"code","source":"# --- DataFrame with non-missing Guest_Popularity_percentage ---\ndf_with_guest_pop = df_train[df_train['Guest_Popularity_percentage'].notna()].copy()\n\n# --- DataFrame with missing Guest_Popularity_percentage ---\ndf_without_guest_pop = df_train[df_train['Guest_Popularity_percentage'].isna()].copy()\ndf_without_guest_pop = df_without_guest_pop.drop(columns=['Guest_Popularity_percentage'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Take a sample of 20,000 samples ---\n\ndf_with_guest_pop_sample = df_with_guest_pop.sample(n=20_000, random_state=42)\n\ndf_without_guest_pop_sample = df_without_guest_pop.sample(n=20_000, random_state=42)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Separating vector and matrix -----\n\nX_with_gp = df_with_guest_pop_sample.drop(columns = 'Listening_Time_minutes')\ny_with_gp = df_with_guest_pop_sample['Listening_Time_minutes']\n\nX_without_gp = df_without_guest_pop_sample.drop(columns = 'Listening_Time_minutes')\ny_without_gp = df_without_guest_pop_sample['Listening_Time_minutes']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Split into train and test sets for two models -----\n\n# With Guest_Popularity_Percentange\nX_with_gp_train, X_with_gp_test, y_with_gp_train, y_with_gp_test = train_test_split(\n    X_with_gp, y_with_gp, test_size=0.30, random_state=42\n)\n\n# Without Guest_Popularity_Percentange\nX_without_gp_train, X_without_gp_test, y_without_gp_train, y_without_gp_test = train_test_split(\n    X_without_gp, y_without_gp, test_size=0.30, random_state=42\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='models'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>7. | Models</strong></div>\n\n<div style=\"font-family: Helvetica; line-height: 1.8; color: #000000; text-align: justify\">\n    \nIn this section, we will <span style=\"background-color: #F6CECE; color: #000000;\"><strong>train several models</strong></span> to identify the one that offers the best performance, as well as the optimal hyperparameters.\n<br>\n<u>To do so, we will use two search methods:</u>\n\n<li>GridSearchCV</li>\n\n<li>Optuna</li>","metadata":{}},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>7.1 | Import libraries üìö</strong></div>","metadata":{}},{"cell_type":"code","source":"# --- Installing Libraries ---\n!pip install xgboost\n!pip install optuna","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----- Preprocessing -----\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# ----- Hyperparameters -----\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport optuna\n\n# ----- Models -----\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# ----- Metrics -----\nfrom sklearn.metrics import mean_squared_error","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>7.2 | Linear Regression</strong></div>","metadata":{}},{"cell_type":"code","source":"# --------------- Train model n¬∞01 with Guest Popularity using linear regression ---------------\n\"\"\"\n\n\n# ----- Pipeline with Linear Regression -----\n\nlr_pipeline_with_gp = make_pipeline(\n    preprocessing_pipeline,\n    LinearRegression()\n)\n\n# ----- Hyperparameters -----\n\nparam_lr_with_gp = {\n    'linearregression__fit_intercept': [True, False],\n}\n\n# ----- GridSearchCV -----\n\ngrid_search_lr_with_gp = GridSearchCV(\n    lr_pipeline_with_gp,\n    param_lr_with_gp,\n    cv=5,\n    scoring='neg_root_mean_squared_error',\n    n_jobs=-1\n)\n\ngrid_search_lr_with_gp.fit(X_with_gp_train, y_with_gp_train)\n\n# ----- Show the best hyperparameters -----\n\npd.DataFrame(grid_search_lr_with_gp.cv_results_).sort_values(\"rank_test_score\").head()\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------- Train model n¬∞02 without Guest Popularity using linear regression ---------------\n\"\"\"\n\n\n# ----- Pipeline with Linear Regression -----\n\nlr_pipeline_without_gp = make_pipeline(\n    preprocessing_pipeline,\n    LinearRegression()\n)\n\n# ----- Hyperparameters -----\n\nparam_lr_without_gp = {\n    'linearregression__fit_intercept': [True, False],\n}\n\n# ----- GridSearchCV -----\n\ngrid_search_lr_without_gp = GridSearchCV(\n    lr_pipeline_without_gp,\n    param_lr_without_gp,\n    cv=5,\n    scoring='neg_root_mean_squared_error',\n    n_jobs=-1\n)\n\ngrid_search_lr_without_gp.fit(X_without_gp_train, y_without_gp_train)\n\n# ----- Show the best hyperparameters -----\n\npd.DataFrame(grid_search_lr_without_gp.cv_results_).sort_values(\"rank_test_score\").head()\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>7.3 | Random Forest</strong></div>","metadata":{}},{"cell_type":"code","source":"# --------------- Train model n¬∞01 with Guest Popularity using RandomForest ---------------\n\"\"\"\n\n\n# ----- Pipeline with RandomForest Regressor -----\n\nrf_pipeline_with_gp = make_pipeline(\n    preprocessing_pipeline,\n    RandomForestRegressor(random_state = 450)\n)\n\n# ----- Hyperparameters -----\n\nparam_rf_with_gp = {\n    'randomforestregressor__n_estimators': [100, 200, 300],\n    'randomforestregressor__max_depth': [2, 5, 7, 15],      \n    'randomforestregressor__min_samples_split': [2, 5, 10],           \n    'randomforestregressor__min_samples_leaf': [1, 4, 7],            \n}\n\n# ----- GridSearchCv -----\n\ngrid_search_rf_with_gp = GridSearchCV(rf_pipeline_with_gp, \n                                      param_rf_with_gp, \n                                      cv = 5, \n                                      scoring='neg_root_mean_squared_error', \n                                      n_jobs=-1)\n\ngrid_search_rf_with_gp.fit(X_with_gp_train, y_with_gp_train)\n\n# ----- Show the best hyperparameters -----\n\npd.DataFrame(grid_search_rf_with_gp.cv_results_).sort_values(\"rank_test_score\").head()\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------- Train model n¬∞02 without Guest Popularity using RandomForest ---------------\n\"\"\"\n\n\n# ----- Pipeline with RandomForest Regressor -----\n\nrf_pipeline_without_gp = make_pipeline(\n    preprocessing_pipeline,\n    RandomForestRegressor(random_state = 450)\n)\n\n# ----- Hyperparameters -----\n\nparam_rf_without_gp = {\n    'randomforestregressor__n_estimators': [100, 200, 300],\n    'randomforestregressor__max_depth': [2, 5, 7, 15],  \n    'randomforestregressor__min_samples_split': [2, 5, 10], \n    'randomforestregressor__min_samples_leaf': [1, 4, 7], \n}\n\n# ----- GridSearchCv -----\n\ngrid_search_rf_without_gp = GridSearchCV(rf_pipeline_without_gp, \n                                      param_rf_without_gp, \n                                      cv = 5, \n                                      scoring='neg_root_mean_squared_error', \n                                      n_jobs=-1)\n\ngrid_search_rf_without_gp.fit(X_without_gp_train, y_without_gp_train)\n\n# ----- Show the best hyperparameters -----\n\npd.DataFrame(grid_search_rf_without_gp.cv_results_).sort_values(\"rank_test_score\").head()\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>7.4 | XGBoost (Optuna)</strong></div>","metadata":{}},{"cell_type":"code","source":"# --------------- Train model n¬∞01 with Guest Popularity using XGBoost and Optuna ---------------\n\"\"\"\ndef objective(trial):\n    \n    # Hyperparameters\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0)\n    }\n\n    # Pipeline\n    model = make_pipeline(\n        preprocessing_pipeline,\n        XGBRegressor(\n            objective=\"reg:squarederror\",\n            random_state=42,\n            n_jobs=-1,\n            **params\n        )\n    )\n\n    # Cross-validation with RMSE\n    scores = cross_val_score(\n        model,\n        X_with_gp_train,\n        y_with_gp_train,\n        scoring=\"neg_root_mean_squared_error\",\n        cv=5,\n        n_jobs=-1\n    )\n\n    return np.mean(scores)\n\n# Create and lunch Optuna\nstudy = optuna.create_study(direction=\"maximize\") \nstudy.optimize(objective, n_trials=100)\n\n# Best parameters\nprint(\"Best trial:\")\nprint(study.best_trial)\nbest_params_with_gp = study.best_trial.params\n\"\"\"","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------- Train model n¬∞02 without Guest Popularity using XGBoost and Optuna ---------------\n\"\"\"\ndef objective(trial):\n    \n    # Hyperparameters\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0)\n    }\n\n    # Pipeline\n    model = make_pipeline(\n        preprocessing_pipeline,\n        XGBRegressor(\n            objective=\"reg:squarederror\",\n            random_state=42,\n            n_jobs=-1,\n            **params\n        )\n    )\n\n    # Cross-validation with RMSE\n    scores = cross_val_score(\n        model,\n        X_without_gp_train,\n        y_without_gp_train,\n        scoring=\"neg_root_mean_squared_error\",\n        cv=5,\n        n_jobs=-1\n    )\n\n    return np.mean(scores)\n\n# Create and lunch Optuna\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n\n# Best parameters\nprint(\"Best trial:\")\nprint(study.best_trial)\nbest_params_without_gp = study.best_trial.params\n\"\"\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>7.5 | Final Model</strong></div>","metadata":{}},{"cell_type":"code","source":"# -------------------- Train the entire train set --------------------\n\n# ----- Separating vector and matrix -----\n\nX_with_gp = df_with_guest_pop.drop(columns = 'Listening_Time_minutes')\ny_with_gp = df_with_guest_pop['Listening_Time_minutes']\n\nX_without_gp = df_without_guest_pop.drop(columns = 'Listening_Time_minutes')\ny_without_gp = df_without_guest_pop['Listening_Time_minutes']\n\n\n# ----- Split into train and test sets for two models -----\n\n# With Guest_Popularity_Percentange (Model n¬∞01)\nX_with_gp_train, X_with_gp_test, y_with_gp_train, y_with_gp_test = train_test_split(\n    X_with_gp, y_with_gp, test_size=0.30, random_state=42\n)\n\n# Without Guest_Popularity_Percentange (Model n¬∞02)\nX_without_gp_train, X_without_gp_test, y_without_gp_train, y_without_gp_test = train_test_split(\n    X_without_gp, y_without_gp, test_size=0.30, random_state=42\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- Pipeline with the final model n¬∞01 (with Guest Popularity) ----------\n\n# Best hyperparameters\nfinal_param_xgb_with_gp = {\n    'n_estimators': 425,\n    'max_depth': 4,\n    'learning_rate': 0.01359661519238633,\n    'subsample': 0.4008805073963276,\n    'colsample_bytree': 0.8325187832312463,\n}\n\n# Final pipeline\npipeline_with_gp = make_pipeline(\n    preprocessing_pipeline,\n    XGBRegressor(random_state = 42, **final_param_xgb_with_gp)\n)\n\n\n# ---------- Pipeline with the final model n¬∞02 (without Guest Popularity) ----------\n\n# Best hyperparameters\nfinal_param_xgb_without_gp = {\n    'n_estimators': 339,\n    'max_depth': 4,\n    'learning_rate': 0.01530999750231462,\n    'subsample': 0.40013910962061794,\n    'colsample_bytree': 0.9824077721256953,\n}\n\n# Final pipeline\npipeline_without_gp = make_pipeline(\n    preprocessing_pipeline,\n    XGBRegressor(random_state = 450, **final_param_xgb_without_gp)\n)\n\n\n# ---------- Train the final models on the entire train set without cross validation ----------\n\n# With Guest Popularity (Model n¬∞01)\npipeline_with_gp.fit(X_with_gp_train, y_with_gp_train)\n\n\n# Without Guest Popularity (Model n¬∞02)\npipeline_without_gp.fit(X_without_gp_train, y_without_gp_train)\n\n\n# ---------- Prediction ----------\n\ny_pred_with_gp = pipeline_with_gp.predict(X_with_gp_test)\ny_pred_without_gp = pipeline_without_gp.predict(X_without_gp_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <div style=\" background-color: #D72729; color:white; padding: 10px; line-height: 1.5;\"><strong>7.6 | Evaluation</strong></div>","metadata":{}},{"cell_type":"code","source":"# ------ Residual ------\nresiduals_with_gp = y_pred_with_gp - y_with_gp_test\nresiduals_with_gp_abs = np.abs(residuals_with_gp)\n\n# ------ Find the interval where 95% of errors are included ------\nX_95_gp = 30\npercentage_gp = 3.88\n\nwhile percentage_gp <= 5 :\n    X_95_gp -= 0.01\n    percentage_gp = np.mean(residuals_with_gp_abs >= X_95_gp) * 100\n    \nprint(np.round(X_95_gp, 3), np.round(percentage_gp, 3))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 6), dpi = 300)\n\n\n# ------------------------------------ Scatterplot of residues ------------------------------------\naxes[0].scatter(y_with_gp_test, residuals_with_gp, alpha=0.02, color = \"#0070C0\")\naxes[0].axhline(0, color=\"red\", linestyle=\"--\", linewidth=2, alpha = 0.5)\n\n\n# Title\nhighlight_textprops = {\"fontsize\":14, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}\naxes[0].set_title(\"Distribution of errors based on \\nthe actual listening duration of a podcast\", \n                  pad=20,\n                  **highlight_textprops)\n\n# Define x and y axis name\nhighlight_textprops1 = [{\"fontsize\":12, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}]\naxes[0].set_xlabel(f\"Real podcast listening time\", **highlight_textprops1[-1])\naxes[0].set_ylabel(f\"Errors\", **highlight_textprops1[-1])\naxes[0].yaxis.labelpad = 20\naxes[0].xaxis.labelpad = 20\n\n# Remove top right frame\naxes[0].spines['top'].set_visible(False)\naxes[0].spines['right'].set_visible(False)\n    \n# Add a frame at bottom left\naxes[0].spines['bottom'].set_linewidth(1.3)\naxes[0].spines['bottom'].set_color('#CAC9CD')\naxes[0].spines['left'].set_linewidth(1.3)\naxes[0].spines['left'].set_color('#CAC9CD')\n    \n# Add grids\naxes[0].grid(axis='x', which='major', alpha=0.5, linestyle='dotted', zorder=1)\naxes[0].grid(axis='y', alpha=0, zorder=2)\n       \n# Change the color of the bars on the x axis\naxes[0].tick_params(axis='x', colors='#CAC9CD', width=1.3)\naxes[0].tick_params(axis='y', colors='#CAC9CD', width=1.3)\n      \n# Change the color of the bar values on the x axis\nfor tick in axes[0].get_xticklabels():\n    tick.set_color('#202020') \n\n# Change color of y-axis scale bar values\nfor tick in axes[0].get_yticklabels():\n    tick.set_color('#202020') \n\n\n\n\n\n# ------------------------------------ KDE of residues ------------------------------------\nsns.kdeplot(residuals_with_gp, ax=axes[1], fill=True, alpha = 0.3)\n    \n# Title\nhighlight_textprops = {\"fontsize\":14, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}\naxes[1].set_title(\"Error distribution\", \n                  pad=20,\n                  **highlight_textprops)\n    \n# D√©finir le nom de l'axe des x et y\nhighlight_textprops1 = [{\"fontsize\":12, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}]\naxes[1].set_xlabel(f\"Errors\", **highlight_textprops1[-1])\naxes[1].set_ylabel(f\"Density\", **highlight_textprops1[-1])    \naxes[1].xaxis.labelpad = 20\naxes[1].yaxis.labelpad = 20\n\n\n# Remove top right frame\naxes[1].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\n\n# Add a frame at bottom left\naxes[1].spines['bottom'].set_linewidth(1.2)\naxes[1].spines['bottom'].set_color('#CAC9CD')\naxes[1].spines['left'].set_linewidth(1.2)\naxes[1].spines['left'].set_color('#CAC9CD')\n\n# Add grids\naxes[1].grid(axis='x', which='major', alpha=0.75, linestyle='dotted', zorder=1)\naxes[1].grid(axis='y', alpha=0, zorder=2)\n\n# Change the color of the bars on the x axis\naxes[1].tick_params(axis='x', colors='#CAC9CD', width=1.2)\naxes[1].tick_params(axis='y', colors='#CAC9CD', width=1.2)\n\n# Change the color of the bar values on the x axis\nfor tick in axes[1].get_xticklabels():\n    tick.set_color('#202020') \n\n# Change color of y-axis scale bar values\nfor tick in axes[1].get_yticklabels():\n    tick.set_color('#202020') \n\n# Add a general title for all graphics\nfig.suptitle(\"Model n¬∞01: Evaluation Performance Report (with)\", fontsize=22, \n             fontweight='semibold', \n             fontname=\"Roboto\",\n             y=1.02)\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.45) \nplt.subplots_adjust(wspace=0.25)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------ Residual ------\nresiduals_without_gp = y_pred_without_gp - y_without_gp_test\nresiduals_without_gp_abs = np.abs(residuals_without_gp)\n\n# ------ Find the interval where 95% of errors are included ------\nX_95 = 30\npercentage = 3.88\n\nwhile percentage <= 5 :\n    X_95 -= 0.01\n    percentage = np.mean(residuals_without_gp_abs >= X_95) * 100\n    \nprint(np.round(X_95, 3), np.round(percentage, 3))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 6), dpi = 300)\n\n\n# ------------------------------------ Scatterplot of residues ------------------------------------\naxes[0].scatter(y_without_gp_test, residuals_without_gp, alpha=0.07, color = \"#0070C0\")\naxes[0].axhline(0, color=\"red\", linestyle=\"--\", linewidth=2, alpha = 0.5)\n\n\n# Title\nhighlight_textprops = {\"fontsize\":14, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}\naxes[0].set_title(\"Distribution of errors based on \\nthe actual listening duration of a podcast\", \n                  pad=20,\n                  **highlight_textprops)\n\n# Define x and y axis name\nhighlight_textprops1 = [{\"fontsize\":12, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}]\naxes[0].set_xlabel(f\"Real podcast listening time\", **highlight_textprops1[-1])\naxes[0].set_ylabel(f\"Errors\", **highlight_textprops1[-1])\naxes[0].yaxis.labelpad = 20\naxes[0].xaxis.labelpad = 20\n\n# Remove top right frame\naxes[0].spines['top'].set_visible(False)\naxes[0].spines['right'].set_visible(False)\n    \n# Add a frame at bottom left\naxes[0].spines['bottom'].set_linewidth(1.3)\naxes[0].spines['bottom'].set_color('#CAC9CD')\naxes[0].spines['left'].set_linewidth(1.3)\naxes[0].spines['left'].set_color('#CAC9CD')\n    \n# Add grids\naxes[0].grid(axis='x', which='major', alpha=0.5, linestyle='dotted', zorder=1)\naxes[0].grid(axis='y', alpha=0, zorder=2)\n       \n# Change the color of the bars on the x axis\naxes[0].tick_params(axis='x', colors='#CAC9CD', width=1.3)\naxes[0].tick_params(axis='y', colors='#CAC9CD', width=1.3)\n      \n# Change the color of the bar values on the x axis\nfor tick in axes[0].get_xticklabels():\n    tick.set_color('#202020') \n\n# Change color of y-axis scale bar values\nfor tick in axes[0].get_yticklabels():\n    tick.set_color('#202020') \n\n\n\n\n\n# ------------------------------------ KDE of residues ------------------------------------\nsns.kdeplot(residuals_without_gp, ax=axes[1], fill=True, alpha = 0.3)\n    \n# Title\nhighlight_textprops = {\"fontsize\":14, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}\naxes[1].set_title(\"Error distribution\", \n                  pad=20,\n                  **highlight_textprops)\n    \n# D√©finir le nom de l'axe des x et y\nhighlight_textprops1 = [{\"fontsize\":12, \"color\":'#262626', \"fontname\": \"Roboto\", \"fontweight\": \"semibold\"}]\naxes[1].set_xlabel(f\"Errors\", **highlight_textprops1[-1])\naxes[1].set_ylabel(f\"Density\", **highlight_textprops1[-1])    \naxes[1].xaxis.labelpad = 20\naxes[1].yaxis.labelpad = 20\n\n\n# Remove top right frame\naxes[1].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\n\n# Add a frame at bottom left\naxes[1].spines['bottom'].set_linewidth(1.2)\naxes[1].spines['bottom'].set_color('#CAC9CD')\naxes[1].spines['left'].set_linewidth(1.2)\naxes[1].spines['left'].set_color('#CAC9CD')\n\n# Add grids\naxes[1].grid(axis='x', which='major', alpha=0.75, linestyle='dotted', zorder=1)\naxes[1].grid(axis='y', alpha=0, zorder=2)\n\n# Change the color of the bars on the x axis\naxes[1].tick_params(axis='x', colors='#CAC9CD', width=1.2)\naxes[1].tick_params(axis='y', colors='#CAC9CD', width=1.2)\n\n# Change the color of the bar values on the x axis\nfor tick in axes[1].get_xticklabels():\n    tick.set_color('#202020') \n\n# Change color of y-axis scale bar values\nfor tick in axes[1].get_yticklabels():\n    tick.set_color('#202020') \n\n# Add a general title for all graphics\nfig.suptitle(\"Model n¬∞02: Evaluation Performance Report (without)\", fontsize=22, \n             fontweight='semibold', \n             fontname=\"Roboto\",\n             y=1.02)\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.45) \nplt.subplots_adjust(wspace=0.25)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------- Evaluation --------------------------------\n\n\n# --- With Guest Popularity ---\n# RMSE\nrmse_with_gp = np.sqrt(np.mean((y_pred_with_gp - y_with_gp_test)**2))\nrmse_with_gp\n\n# MAE\nmae_with_gp = np.mean(np.abs(y_pred_with_gp - y_with_gp_test))\nmae_with_gp\n\n# R2\nr2_with_gp = ((np.var(y_with_gp_test) - np.var(y_with_gp_test - y_pred_with_gp)) / np.var(y_with_gp_test)) * 100\nr2_with_gp\n\n\n# --- Without Guest Popularity ---\n# RMSE\nrmse_without_gp = np.sqrt(np.mean((y_pred_without_gp - y_without_gp_test)**2))\nrmse_without_gp\n\n# MAE\nmae_without_gp = np.mean(np.abs(y_pred_without_gp - y_without_gp_test))\nmae_without_gp\n\n# R2\nr2_without_gp = ((np.var(y_without_gp_test) - np.var(y_without_gp_test - y_pred_without_gp)) / np.var(y_without_gp_test)) * 100\nr2_without_gp\n\n\n# --- Table ---\nresults = pd.DataFrame({\n    \"Model\": [\"With Guest Popularity\", \"Without Guest Popularity\"],\n    \"RMSE\": [rmse_with_gp, rmse_without_gp],\n    \"MAE\": [mae_with_gp, mae_without_gp],\n    \"R¬≤\": [r2_with_gp, r2_without_gp]\n})\n\nresults.round(2)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-family: Helvetica; line-height: 1.75; color: #000000; text-align: justify\">\n\n    \nWe observe that both models have <u>similar</u> performance.\n<br><br>\nOn <strong>average</strong>, the two models make an error of <strong>¬±13.3 minutes</strong>. For example, if our model predicts that a user will listen to the podcast for <strong>60</strong> minutes, the actual duration could reasonably fall between <strong>46.7 and 73.3 minutes</strong>.\n<br>\nFrom the scatter plots, we can see that both models <strong>are biased toward</strong> users who listen to a podcast very briefly (less than 15 minutes) and those who listen for a long time (more than 80 minutes).\n<br>\n<strong>95%</strong> of the errors fall within the range [-27.4; +27.4] minutes. The probability that the models make an error greater or less than 27.4 minutes is 5%.\n<br><br>\nTo summarize, <span style=\"background-color: #F6CECE; color: #000000;\"><strong>both models have average to poor performance</strong></span>. The RMSE, MAE, and R¬≤ are moderate, but the models are biased.    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id='submission'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>8. | Submission</strong></div>","metadata":{}},{"cell_type":"code","source":"# ------------- Train the models over the entire dataset -------------\n\nX = df_train.drop(columns = 'Listening_Time_minutes')\ny = df_train['Listening_Time_minutes']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------- ConditionalModelRouter: Dynamic Prediction Based on Feature Availability -------------\n\nclass ConditionalModelRouter(BaseEstimator, RegressorMixin):\n    def __init__(self, model_with_gp, model_without_gp):\n        self.model_with_gp = model_with_gp\n        self.model_without_gp = model_without_gp\n\n    def fit(self, X, y):\n        \n        mask = ~X[\"Guest_Popularity_percentage\"].isna() # Return true or false for each index\n        self.model_with_gp.fit(X[mask], y[mask]) # Train only for data with Guest Popularity\n        self.model_without_gp.fit(X[~mask].drop(columns=[\"Guest_Popularity_percentage\"]), y[~mask]) # Train only for data without Guest Popularity\n        return self\n\n    def predict(self, X):\n        X = X.copy()\n        mask = ~X[\"Guest_Popularity_percentage\"].isna() # Return true or false for each index\n        y_pred = np.empty(len(X))\n        \n        y_pred[mask] = self.model_with_gp.predict(X[mask]) # Predict only for data with Guest Popularity\n        y_pred[~mask] = self.model_without_gp.predict(X[~mask].drop(columns=[\"Guest_Popularity_percentage\"])) # Predict only for data without Guest Popularity\n        return y_pred","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------- Testing the ConditionalModelRouter -------------\n\n# Model\nfrom sklearn.dummy import DummyRegressor\n\n# Data\nX_router_test =  pd.DataFrame({\n    \"Feature1\": [1, 2, 3, 4],\n    \"Guest_Popularity_percentage\": [0.8, np.nan, 0.5, np.nan]\n})\n\ny_router_test = np.array([10, 20, 100, 30])\n\n# Model\nmodel_with = DummyRegressor(strategy=\"mean\")\nmodel_without = DummyRegressor(strategy=\"constant\", constant=42)\n\n# Router\nrouter_test = ConditionalModelRouter(model_with_gp=model_with, model_without_gp=model_without)\nrouter_test.fit(X_router_test, y_router_test)\n\n# Preds\npreds = router_test.predict(X_router_test)\nprint(\"Pr√©dictions :\", preds)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- Pipeline with the final model n¬∞01 (with Guest Popularity) ----------\n\n# Best hyperparameters\nfinal_param_xgb_with_gp = {\n    'n_estimators': 425,\n    'max_depth': 4,\n    'learning_rate': 0.01359661519238633,\n    'subsample': 0.4008805073963276,\n    'colsample_bytree': 0.8325187832312463,\n}\n\n# Final pipeline\npipeline_with_gp = make_pipeline(\n    preprocessing_pipeline,\n    XGBRegressor(random_state = 450, **final_param_xgb_with_gp)\n)\n\n\n# ---------- Pipeline with the final model n¬∞02 (without Guest Popularity) ----------\n\n# Best hyperparameters\nfinal_param_xgb_without_gp = {\n    'n_estimators': 339,\n    'max_depth': 4,\n    'learning_rate': 0.01530999750231462,\n    'subsample': 0.40013910962061794,\n    'colsample_bytree': 0.9824077721256953,\n}\n\n# Final pipeline\npipeline_without_gp = make_pipeline(\n    preprocessing_pipeline,\n    XGBRegressor(random_state = 450, **final_param_xgb_without_gp)\n)\n\n\n# ---------- Adding both models to a router ----------\n\nrouter = ConditionalModelRouter(\n    model_with_gp=pipeline_with_gp,\n    model_without_gp=pipeline_without_gp\n)\n\n\n# ---------- Train the models over the entire dataset ----------\n\nrouter.fit(X, y)\n\n\n# ---------- Prediction ----------\n\ny_deployment = router.predict(df_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Submission\nsubmission = pd.DataFrame({'id': df_test.index , 'Listening_Time_minutes': y_deployment.round(3)})\nsubmission.to_csv(\"podcast_submission.csv\", index = False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id='conclusion'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>9. | Conclusion</strong></div>\n\n\n<div style=\"font-family: Helvetica; line-height: 1.75; color: #000000; text-align: justify\">\n\n    \nWe have completed our project aimed at predicting how long a user will listen to a podcast.\n<br><br>\n<strong><u>In this project, we learned to:</u></strong>\n\n<li>Handle missing values efficiently</li>\n\n<li>Better understand pipelines</li>\n\n<li>Build two models and use a routing pipeline</li>\n\n<li>Use Optuna</li>\n<br><br>\n<strong><u>Areas for improvement include:</u></strong>\n\n<li>Feature engineering</li>\n\n<li>Pipelines</li>\n\n<li>Evaluation after cross-validation</li>\n\n<li>The routing pipeline</li>\n","metadata":{}},{"cell_type":"markdown","source":"<a id='references'></a>\n# <div style=\" background-color:#162B48; color:white; padding: 12px; line-height: 1.5;\"><strong>10. | References</strong>\n    \n<div style=\"font-family: Helvetica; line-height: 2; color: #000000; text-align: justify\">\n\n<ul><u><strong>Kaggle Notebook</strong> üìö</u>\n        <li><a style=\"color: #462F53\" href=\"https://www.kaggle.com/code/vasusoni/predict-podcast-ensemble\">RAPIDS SVC w/ Feature Engineering - [LB 0.856] by Vasu Soni</a></li>\n    </ul>\n</div> ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}